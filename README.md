# BGP-LLaMA: Fine-tuning Open-Source LLM on BGP Routing Knowledge and Analysis
We present the first open-source LLM specialized in the BGP routing analytics- BGP-LLaMA, instruction-tuned LLaMA-13B model. We built BGP-LLaMA by fine-tuning LLaMA with our own BGP training data that were collected from Internet standard documents or papers, and generated by self-instruction of OpenAI‚Äôs Chat-GPT. BGP-LLaMA can answer to BGP routing questions and can automate manual BGP analysis tasks of BGP routing data. 

## Updates:
- Oct 17, 2023: We have updated our knowledge dataset, which includes additional instruction-input-ouput sets in topics it previously underformed and more elaborate examples for the outputs. The update enhances the accuracy and relevance of the responses generated by the model.

- Nov 1, 2023: Fine-tuning with the newly updated dataset. We have optimized the model's parameters to strike the optimal balance between training time and performance. The training time from 22 hours to 3.6 hours.

## Navigation

### üìÑ `llama_bpg_finetune.ipynb`

This Jupyter notebook is the primary script used to fine-tune the BGP-LLaMA model. If you're looking to understand the core training process, this is the place to start.

### üõ†Ô∏è `generate_instruction.py`

The script is originally sourced from [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca). This script is instrumental in expanding the training dataset, contributing significantly to the diversity of data used to train the model. ["Self-instruct"](https://arxiv.org/abs/2212.10560) crafted to expand the training dataset. It plays a crucial role in ensuring that the model has a diverse set of data for training.

### üìù Prompts

- `prompt_knowledge.txt`: A set of prompts to generate BGP domain knowledge.
  
- `prompt_pybgpstream.txt`: An additional prompt to generate BGP analysis with PyBGPStream library.

### üå± `seed_tasks` Folder

Manual seed tasks which are vital for generating new instructions. 

### üìä `evaluation` Folder

Evaluation sets for both knowledge and code analysis evaluations. These sets are designed to assess the performance of the BGP-LLaMA once it has been fine-tuned.

## Fine-Tuning parameters

| Parameter                 | Value                   |
|---------------------------|-------------------------|
| Training steps            | 5,000                  |
| Learning rate             | 1e-4                    |
| Global batch size         | 4                       |
| Warm-up ratio             | 0.05                    |
| LoRA rank                 | 64                      |
| LoRA $\alpha$             | 16                      |
| LoRA dropout rate         | 0.1                     |
| Bias configuration        | excluded                |
| Total trainable parameters| 52,428,800              |
| Total training time       | 3.6 hours                |
| Training hardware         | 2 NVIDIA RTX A6000 48GB |
