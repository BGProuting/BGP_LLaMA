# BGP-LLaMA: Fine-tuning Open-Source LLM on BGP Routing Knowledge and Analysis
We present the first open-source LLM specialized in the BGP routing analytics- BGP-LLaMA, instruction-tuned LLaMA-13B model. We built BGP-LLaMA by fine-tuning LLaMA with our own BGP training data that were collected from Internet standard documents or papers, and generated by self-instruction of OpenAI‚Äôs Chat-GPT. BGP-LLaMA can answer to BGP routing questions and can automate manual BGP analysis tasks of BGP routing data. 

## Navigation

### üìÑ `llama_bpg_finetune.ipynb`

This Jupyter notebook is the primary script used to fine-tune the BGP-LLaMA model. If you're looking to understand the core training process, this is the place to start.

### üõ†Ô∏è `generate_instruction.py`

The script is originally sourced from Stanford Alpaca [script](https://github.com/tatsu-lab/stanford_alpaca). This script is instrumental in expanding the training dataset, contributing significantly to the diversity of data used to train the model. "self-instruct" [script](https://arxiv.org/abs/2212.10560) crafted to expand the training dataset. It plays a crucial role in ensuring that the model has a diverse set of data for training.

### üìù Prompts

- `prompt_knowledge.txt`: A set of prompts to generate BGP domain knowledge.
  
- `prompt_pybgpstream.txt`: An additional prompt to generate BGP analysis with PyBGPStream library.

### üå± `seed_tasks` Folder

Manual seed tasks which are vital for generating new instructions. 

### üìä `evaluation` Folder

Evaluation sets for both knowledge and code analysis evaluations. These sets are designed to assess the performance of the BGP-LLaMA once it has been fine-tuned.

## Fine-Tuning parameters

| Parameter                 | Value                   |
|---------------------------|-------------------------|
| Training steps            | 30,000                  |
| Learning rate             | \(1 \times 10^{-4}\)    |
| Global batch size         | 4                       |
| Warm-up ratio             | 0.05                    |
| LoRA rank                 | 64                      |
| LoRA $\alpha$             | 16                      |
| LoRA dropout rate         | 0.1                     |
| Bias configuration        | excluded                |
| Total trainable parameters| 52,428,800              |
| Total training time       | 22 hours                |
| Training hardware         | 2 NVIDIA RTX A6000 48GB |
